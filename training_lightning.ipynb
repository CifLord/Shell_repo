{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from Models.Gemformer import Gemformer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ocpmodels.datasets import LmdbDataset\n",
    "from torch.utils.data import random_split\n",
    "import torch_geometric.loader as geom_loader\n",
    "import torch_geometric.data as data\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor,ModelCheckpoint\n",
    "from typing import Any\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict={}\n",
    "def create_model(model_name,model_hparams):\n",
    "    if model_name in model_dict:\n",
    "        return model_dict[model_name](**model_hparams)\n",
    "    else:\n",
    "        assert False, f\"Unknown model name \\\"{model_name}\\\".Available models are: {str(model_dict.keys())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoTransformer_Traniner(pl.LightningModule):\n",
    "    ''''pytorch lightning'''\n",
    "    def __init__(self,model_name, model_hparams, optimizer_name, optimizer_hparams,**model_kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.optimizer_name=optimizer_name\n",
    "        self.optimizer_hparams=optimizer_hparams\n",
    "        self.model=create_model(model_name,model_hparams)\n",
    "        self.loss_module=nn.MSELoss()\n",
    "\n",
    "    def forward(self,data,mode=\"train\"):\n",
    "        # x,edge_index,batch_idx=data.latent,data.edge_index,data.batch\n",
    "        # print(data)\n",
    "        x=self.model(data)\n",
    "        x=x.squeeze(dim=-1)\n",
    "        preds=x.float()\n",
    "        loss=self.loss_module(data.y_relaxed,x)\n",
    "        acc=abs(data.y_relaxed-preds)\n",
    "\n",
    "        return loss,acc\n",
    "    \n",
    "    def configure_optimizers(self) -> Any:\n",
    "\n",
    "        if self.optimizer_name == \"Adam\":\n",
    "            optimizer=optim.AdamW(\n",
    "                self.parameters(),**self.hparams.optimizer_hparams\n",
    "            )\n",
    "        scheduler=optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer,milestones=[20,30],gamma=0.1\n",
    "        )    \n",
    "        # return super().configure_optimizers()\n",
    "        return [optimizer],[scheduler]\n",
    "\n",
    "\n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        loss,acc=self.forward(batch,mode=\"train\")\n",
    "        self.log(\"train_loss\",loss)\n",
    "        self.log(\"train_mae\",acc)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        _,acc=self.forward(batch,mode=\"val\")\n",
    "        self.log(\"val_mae\",acc)\n",
    "\n",
    "    def test_step(self,batch,batch_idx):\n",
    "        _,acc=self.forward(batch,mode=\"test\")\n",
    "        self.log(\"test_mae\",acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ReverseDataset(data.Dataset):\n",
    "\n",
    "#     def __init__(self, data,size):\n",
    "#         super().__init__()\n",
    "#         # self.size = data\n",
    "#         self.data = data\n",
    "#         self.size=size\n",
    "  \n",
    "#     def __len__(self):\n",
    "#         return self.size\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         inp_data = self.data[idx]\n",
    "#         labels = inp_data.y_relaxed\n",
    "#         return inp_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3g8sap4a) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-AdamW</td><td>████████████████▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>train_acc</td><td>▃▁▁▇▃▃▂▃▂▅▄▃▃▁▅█▃▅▄▃▆▂▂▆▂▁▂▂▄▂▂▃▁▃█▃▃▁▄▂</td></tr><tr><td>train_loss</td><td>▂▁▁▇▂▂▁▂▁▃▂▁▂▁▃▇▂▃▂▂▅▁▁▅▁▁▁▁▂▁▁▁▁▁█▂▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>lr-AdamW</td><td>1e-05</td></tr><tr><td>test_acc</td><td>1.74039</td></tr><tr><td>train_acc</td><td>0.31811</td></tr><tr><td>train_loss</td><td>0.10119</td></tr><tr><td>trainer/global_step</td><td>76150</td></tr><tr><td>val_acc</td><td>1.50847</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">upbeat-energy-2</strong>: <a href=\"https://wandb.ai/moxx799/uncategorized/runs/3g8sap4a\" target=\"_blank\">https://wandb.ai/moxx799/uncategorized/runs/3g8sap4a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230609_005736-3g8sap4a\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3g8sap4a). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\lhuang37\\Desktop\\Build_DNN\\wandb\\run-20230609_014122-1gmtcaie</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/moxx799/uncategorized/runs/1gmtcaie\" target=\"_blank\">kind-forest-3</a></strong> to <a href=\"https://wandb.ai/moxx799/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH=\"./checkpoints\"\n",
    "\n",
    "dataset=LmdbDataset({\"src\":\"Data/eoh_t.lmdb\"})\n",
    "\n",
    "train_length = int(0.8 * len(dataset))\n",
    "val_length = len(dataset) - train_length\n",
    "\n",
    "# Split the dataset into train and validation\n",
    "train_dataset, val_dataset =random_split(dataset, [train_length, val_length])\n",
    "# train_dataset=ReverseDataset(train_dataset,train_length)\n",
    "# val_dataset=ReverseDataset(val_dataset,val_length)\n",
    "train_loader = geom_loader.DataLoader(train_dataset, batch_size=1)\n",
    "val_loader = geom_loader.DataLoader(val_dataset, batch_size=1)\n",
    "wandb.init()\n",
    "wandb_logger = WandbLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp_data, labels = train_loader\n",
    "# print(\"Input data:\", inp_data)\n",
    "# print(\"Labels:    \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name,save_name=None,**kwargs):\n",
    "\n",
    "    if save_name is None:\n",
    "        save_name=model_name\n",
    "    \n",
    "    trainer=pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH,save_name),\n",
    "                       accelerator='gpu',\n",
    "                       devices=1,\n",
    "                       max_epochs=50,\n",
    "                       callbacks=[ModelCheckpoint(save_weights_only=True,mode=\"max\",monitor=\"val_mae\"),\n",
    "                                  LearningRateMonitor(\"epoch\")],\n",
    "                       enable_progress_bar=True,\n",
    "                       logger=wandb_logger)\n",
    "    \n",
    "    trainer.logger._log_graph=True\n",
    "    trainer.logger._default_hp_metric=None\n",
    "    pretrained_filename=os.path.join(CHECKPOINT_PATH,save_name+\".ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        model=GeoTransformer_Traniner().load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)\n",
    "        model=GeoTransformer_Traniner(model_name=\"Gemformer\",**kwargs)\n",
    "        trainer.fit(model,train_loader,val_loader)\n",
    "        model=GeoTransformer_Traniner.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    val_result=trainer.test(model,val_loader,verbose=False)\n",
    "    # test_result=trainer.test(model,test_loader,verbose=False)\n",
    "    # result={\"test\":test_result[0][\"test_acc\"],\"val\":val_result[0][\"test_acc\"]}\n",
    "\n",
    "    return model,val_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type      | Params\n",
      "------------------------------------------\n",
      "0 | model       | Gemformer | 68.2 K\n",
      "1 | loss_module | MSELoss   | 0     \n",
      "------------------------------------------\n",
      "68.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "68.2 K    Total params\n",
      "0.273     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  56%|█████▋    | 857/1523 [53:55<41:54,  3.78s/it, v_num=jwli]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "%%capture out\n",
    "model_dict['Gemformer']=Gemformer\n",
    "gemformer_model,gemformer_results=train_model(model_name=\"Gemformer\",\n",
    "                                              model_hparams={\"num_heads\":1,\n",
    "                                                             \"emb_size_in\":256,\n",
    "                                                             \"emb_size_trans\":64},\n",
    "                                              optimizer_name=\"Adam\",\n",
    "                                              optimizer_hparams={\"lr\":1e-3,\n",
    "                                                                 \"weight_decay\":1e-4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "import lmdb\n",
    "import bisect\n",
    "\n",
    "class LmdbDataset(Dataset):\n",
    "    r\"\"\"Dataset class to load from LMDB files containing relaxation\n",
    "    trajectories or single point computations.\n",
    "\n",
    "    Useful for Structure to Energy & Force (S2EF), Initial State to\n",
    "    Relaxed State (IS2RS), and Initial State to Relaxed Energy (IS2RE) tasks.\n",
    "\n",
    "    Args:\n",
    "            config (dict): Dataset configuration\n",
    "            transform (callable, optional): Data transform function.\n",
    "                    (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, transform=None):\n",
    "        super(LmdbDataset, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        assert not self.config.get(\n",
    "            \"train_on_oc20_total_energies\", False\n",
    "        ), \"For training on total energies set dataset=oc22_lmdb\"\n",
    "\n",
    "        self.path = Path(self.config[\"src\"])\n",
    "        if not self.path.is_file():\n",
    "            db_paths = sorted(self.path.glob(\"*.lmdb\"))\n",
    "            assert len(db_paths) > 0, f\"No LMDBs found in '{self.path}'\"\n",
    "\n",
    "            self.metadata_path = self.path / \"metadata.npz\"\n",
    "\n",
    "            self._keys, self.envs = [], []\n",
    "            for db_path in db_paths:\n",
    "                self.envs.append(self.connect_db(db_path))\n",
    "                length = pickle.loads(\n",
    "                    self.envs[-1].begin().get(\"length\".encode(\"ascii\"))\n",
    "                )\n",
    "                self._keys.append(list(range(length)))\n",
    "\n",
    "            keylens = [len(k) for k in self._keys]\n",
    "            self._keylen_cumulative = np.cumsum(keylens).tolist()\n",
    "            self.num_samples = sum(keylens)\n",
    "        else:\n",
    "            self.metadata_path = self.path.parent / \"metadata.npz\"\n",
    "            self.env = self.connect_db(self.path)\n",
    "            self._keys = [\n",
    "                f\"{j}\".encode(\"ascii\")\n",
    "                for j in range(self.env.stat()[\"entries\"])\n",
    "            ]\n",
    "            self.num_samples = len(self._keys)\n",
    "\n",
    "        # If specified, limit dataset to only a portion of the entire dataset\n",
    "        # total_shards: defines total chunks to partition dataset\n",
    "        # shard: defines dataset shard to make visible\n",
    "        self.sharded = False\n",
    "        if \"shard\" in self.config and \"total_shards\" in self.config:\n",
    "            self.sharded = True\n",
    "            self.indices = range(self.num_samples)\n",
    "            # split all available indices into 'total_shards' bins\n",
    "            self.shards = np.array_split(\n",
    "                self.indices, self.config.get(\"total_shards\", 1)\n",
    "            )\n",
    "            # limit each process to see a subset of data based off defined shard\n",
    "            self.available_indices = self.shards[self.config.get(\"shard\", 0)]\n",
    "            self.num_samples = len(self.available_indices)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # if sharding, remap idx to appropriate idx of the sharded set\n",
    "        if self.sharded:\n",
    "            idx = self.available_indices[idx]\n",
    "        if not self.path.is_file():\n",
    "            # Figure out which db this should be indexed from.\n",
    "            db_idx = bisect.bisect(self._keylen_cumulative, idx)\n",
    "            # Extract index of element within that db.\n",
    "            el_idx = idx\n",
    "            if db_idx != 0:\n",
    "                el_idx = idx - self._keylen_cumulative[db_idx - 1]\n",
    "            assert el_idx >= 0\n",
    "\n",
    "            # Return features.\n",
    "            datapoint_pickled = (\n",
    "                self.envs[db_idx]\n",
    "                .begin()\n",
    "                .get(f\"{self._keys[db_idx][el_idx]}\".encode(\"ascii\"))\n",
    "            )\n",
    "            data_object = pyg2_data_transform(pickle.loads(datapoint_pickled))\n",
    "            data_object.id = f\"{db_idx}_{el_idx}\"\n",
    "        else:\n",
    "            datapoint_pickled = self.env.begin().get(self._keys[idx])\n",
    "            data_object = pyg2_data_transform(pickle.loads(datapoint_pickled))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            data_object = self.transform(data_object)\n",
    "\n",
    "        return data_object\n",
    "\n",
    "    def connect_db(self, lmdb_path=None):\n",
    "        env = lmdb.open(\n",
    "            str(lmdb_path),\n",
    "            subdir=False,\n",
    "            readonly=True,\n",
    "            lock=False,\n",
    "            readahead=False,\n",
    "            meminit=False,\n",
    "            max_readers=1,\n",
    "        )\n",
    "        return env\n",
    "\n",
    "    def close_db(self):\n",
    "        if not self.path.is_file():\n",
    "            for env in self.envs:\n",
    "                env.close()\n",
    "        else:\n",
    "            self.env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocp-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

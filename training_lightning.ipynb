{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from Models.Gemformer import Gemformer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ocpmodels.datasets import LmdbDataset\n",
    "from torch.utils.data import random_split\n",
    "import torch_geometric.loader as geom_loader\n",
    "import torch_geometric.data as data\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor,ModelCheckpoint\n",
    "from typing import Any\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict={}\n",
    "def create_model(model_name,model_hparams):\n",
    "    if model_name in model_dict:\n",
    "        return model_dict[model_name](**model_hparams)\n",
    "    else:\n",
    "        assert False, f\"Unknown model name \\\"{model_name}\\\".Available models are: {str(model_dict.keys())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoTransformer_Traniner(pl.LightningModule):\n",
    "    ''''pytorch lightning'''\n",
    "    def __init__(self,model_name, model_hparams, optimizer_name, optimizer_hparams,**model_kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.optimizer_name=optimizer_name\n",
    "        self.optimizer_hparams=optimizer_hparams\n",
    "        self.model=create_model(model_name,model_hparams)\n",
    "        self.loss_module=nn.MSELoss()\n",
    "\n",
    "    def forward(self,data,mode=\"train\"):\n",
    "        x,edge_index,batch_idx=data.latent,data.edge_index,data.batch\n",
    "        # print(data)\n",
    "        x=self.model(x,batch_idx)\n",
    "        x=x.squeeze(dim=-1)\n",
    "        preds=x.float()\n",
    "        loss=self.loss_module(data.y_relaxed,x)\n",
    "        acc=abs(data.y_relaxed-preds)\n",
    "\n",
    "        return loss,acc\n",
    "    \n",
    "    def configure_optimizers(self) -> Any:\n",
    "\n",
    "        if self.optimizer_name == \"Adam\":\n",
    "            optimizer=optim.AdamW(\n",
    "                self.parameters(),**self.hparams.optimizer_hparams\n",
    "            )\n",
    "        scheduler=optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer,milestones=[20,30],gamma=0.1\n",
    "        )    \n",
    "        # return super().configure_optimizers()\n",
    "        return [optimizer],[scheduler]\n",
    "\n",
    "\n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        loss,acc=self.forward(batch,mode=\"train\")\n",
    "        self.log(\"train_loss\",loss)\n",
    "        self.log(\"train_mae\",acc)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        _,acc=self.forward(batch,mode=\"val\")\n",
    "        self.log(\"val_mae\",acc)\n",
    "\n",
    "    def test_step(self,batch,batch_idx):\n",
    "        _,acc=self.forward(batch,mode=\"test\")\n",
    "        self.log(\"test_mae\",acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ReverseDataset(data.Dataset):\n",
    "\n",
    "#     def __init__(self, data,size):\n",
    "#         super().__init__()\n",
    "#         # self.size = data\n",
    "#         self.data = data\n",
    "#         self.size=size\n",
    "  \n",
    "#     def __len__(self):\n",
    "#         return self.size\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         inp_data = self.data[idx]\n",
    "#         labels = inp_data.y_relaxed\n",
    "#         return inp_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH=\"./checkpoints\"\n",
    "\n",
    "dataset=LmdbDataset({\"src\":\"Data/eoh_t.lmdb\"})\n",
    "\n",
    "train_length = int(0.8 * len(dataset))\n",
    "val_length = len(dataset) - train_length\n",
    "\n",
    "# Split the dataset into train and validation\n",
    "train_dataset, val_dataset =random_split(dataset, [train_length, val_length])\n",
    "# train_dataset=ReverseDataset(train_dataset,train_length)\n",
    "# val_dataset=ReverseDataset(val_dataset,val_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp_data, labels = train_loader\n",
    "# print(\"Input data:\", inp_data)\n",
    "# print(\"Labels:    \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name,save_name=None,**kwargs):\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    if save_name is None:\n",
    "        save_name=model_name\n",
    "    \n",
    "    trainer=pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH,save_name),\n",
    "                       accelerator='cuda',\n",
    "                       devices=-1,\n",
    "                       max_epochs=50,\n",
    "                       callbacks=[ModelCheckpoint(save_weights_only=True,mode=\"min\",monitor=\"val_mae\"),\n",
    "                                  LearningRateMonitor(\"epoch\")],\n",
    "                       enable_progress_bar=True,\n",
    "                       logger=wandb_logger)\n",
    "    \n",
    "    trainer.logger._log_graph=True\n",
    "    trainer.logger._default_hp_metric=None\n",
    "    pretrained_filename=os.path.join(CHECKPOINT_PATH,save_name+\".ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        model=GeoTransformer_Traniner().load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)\n",
    "        model=GeoTransformer_Traniner(model_name=\"Gemformer\",**kwargs)\n",
    "        trainer.fit(model,train_loader,val_loader)\n",
    "        model=GeoTransformer_Traniner.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    val_result=trainer.test(model,val_loader,verbose=False)\n",
    "    # test_result=trainer.test(model,test_loader,verbose=False)\n",
    "    # result={\"test\":test_result[0][\"test_acc\"],\"val\":val_result[0][\"test_acc\"]}\n",
    "\n",
    "    return model,val_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1gythqso) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">solar-oath-9</strong>: <a href=\"https://wandb.ai/moxx799/Shell_repo/runs/1gythqso\" target=\"_blank\">https://wandb.ai/moxx799/Shell_repo/runs/1gythqso</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230616_145603-1gythqso\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1gythqso). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\lhuang37\\Desktop\\Shell_trans\\Shell_repo\\wandb\\run-20230616_150003-1zlzrklc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/moxx799/Shell_repo/runs/1zlzrklc\" target=\"_blank\">solar-dew-10</a></strong> to <a href=\"https://wandb.ai/moxx799/Shell_repo\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "wandb.init()\n",
    "wandb_logger = WandbLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type      | Params\n",
      "------------------------------------------\n",
      "0 | model       | Gemformer | 68.2 K\n",
      "1 | loss_module | MSELoss   | 0     \n",
      "------------------------------------------\n",
      "68.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "68.2 K    Total params\n",
      "0.273     Total estimated model params size (MB)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'torch_geometric.data.batch.DataBatch'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lhuang37\\Desktop\\Shell_trans\\Shell_repo\\training_lightning.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/training_lightning.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_dict[\u001b[39m'\u001b[39m\u001b[39mGemformer\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39mGemformer\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/training_lightning.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m gemformer_model,gemformer_results\u001b[39m=\u001b[39mtrain_model(model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mGemformer\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/training_lightning.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                               model_hparams\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mnum_heads\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m1\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/training_lightning.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                                              \u001b[39m\"\u001b[39;49m\u001b[39memb_size_in\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m256\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/training_lightning.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                                              \u001b[39m\"\u001b[39;49m\u001b[39memb_size_trans\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m64\u001b[39;49m},\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/training_lightning.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                                               optimizer_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mAdam\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/training_lightning.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                               optimizer_hparams\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m1e-3\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/training_lightning.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                                                  \u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m1e-4\u001b[39;49m})\n",
      "\u001b[1;32mc:\\Users\\lhuang37\\Desktop\\Shell_trans\\Shell_repo\\training_lightning.ipynb Cell 10\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model_name, save_name, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/training_lightning.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     pl\u001b[39m.\u001b[39mseed_everything(\u001b[39m42\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/training_lightning.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     model\u001b[39m=\u001b[39mGeoTransformer_Traniner(model_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGemformer\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/training_lightning.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     trainer\u001b[39m.\u001b[39;49mfit(model,train_loader,val_loader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/training_lightning.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     model\u001b[39m=\u001b[39mGeoTransformer_Traniner\u001b[39m.\u001b[39mload_from_checkpoint(trainer\u001b[39m.\u001b[39mcheckpoint_callback\u001b[39m.\u001b[39mbest_model_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/training_lightning.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m val_result\u001b[39m=\u001b[39mtrainer\u001b[39m.\u001b[39mtest(model,val_loader,verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:520\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    518\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[0;32m    519\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m--> 520\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    521\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    522\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:559\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[0;32m    550\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[0;32m    551\u001b[0m )\n\u001b[0;32m    553\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    554\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    555\u001b[0m     ckpt_path,\n\u001b[0;32m    556\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    557\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    558\u001b[0m )\n\u001b[1;32m--> 559\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    561\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    562\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    932\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    933\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    934\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 935\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m    937\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    938\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    940\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:976\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m    975\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m--> 976\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[0;32m    977\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m    978\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1005\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1002\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1004\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1005\u001b[0m val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m   1007\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1009\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[0;32m    176\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:108\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 108\u001b[0m         batch, batch_idx, dataloader_idx \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(data_fetcher)\n\u001b[0;32m    109\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[0;32m    110\u001b[0m         \u001b[39mif\u001b[39;00m previous_dataloader_idx \u001b[39m!=\u001b[39m dataloader_idx:\n\u001b[0;32m    111\u001b[0m             \u001b[39m# the dataloader has changed, notify the logger connector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:136\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[0;32m    134\u001b[0m     \u001b[39m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader_iter)\n\u001b[0;32m    137\u001b[0m         \u001b[39m# consume the batch we just fetched\u001b[39;00m\n\u001b[0;32m    138\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\loops\\fetchers.py:150\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher._fetch_next_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_profiler()\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[0;32m    151\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop_profiler()\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:276\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    275\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator)\n\u001b[0;32m    277\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator, _Sequential):\n\u001b[0;32m    278\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\utilities\\combined_loader.py:122\u001b[0m, in \u001b[0;36m_Sequential.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 122\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterators[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_idx])\n\u001b[0;32m    123\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_idx\n\u001b[0;32m    124\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    651\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 652\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    655\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    656\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    691\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    693\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    694\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:183\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m             \u001b[39m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m    181\u001b[0m             \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n\u001b[1;32m--> 183\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem_type))\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'torch_geometric.data.batch.DataBatch'>"
     ]
    }
   ],
   "source": [
    "%%capture out\n",
    "model_dict['Gemformer']=Gemformer\n",
    "gemformer_model,gemformer_results=train_model(model_name=\"Gemformer\",\n",
    "                                              model_hparams={\"num_heads\":1,\n",
    "                                                             \"emb_size_in\":256,\n",
    "                                                             \"emb_size_trans\":64},\n",
    "                                              optimizer_name=\"Adam\",\n",
    "                                              optimizer_hparams={\"lr\":1e-3,\n",
    "                                                                 \"weight_decay\":1e-4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "import lmdb\n",
    "import bisect\n",
    "\n",
    "class LmdbDataset(Dataset):\n",
    "    r\"\"\"Dataset class to load from LMDB files containing relaxation\n",
    "    trajectories or single point computations.\n",
    "\n",
    "    Useful for Structure to Energy & Force (S2EF), Initial State to\n",
    "    Relaxed State (IS2RS), and Initial State to Relaxed Energy (IS2RE) tasks.\n",
    "\n",
    "    Args:\n",
    "            config (dict): Dataset configuration\n",
    "            transform (callable, optional): Data transform function.\n",
    "                    (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, transform=None):\n",
    "        super(LmdbDataset, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        assert not self.config.get(\n",
    "            \"train_on_oc20_total_energies\", False\n",
    "        ), \"For training on total energies set dataset=oc22_lmdb\"\n",
    "\n",
    "        self.path = Path(self.config[\"src\"])\n",
    "        if not self.path.is_file():\n",
    "            db_paths = sorted(self.path.glob(\"*.lmdb\"))\n",
    "            assert len(db_paths) > 0, f\"No LMDBs found in '{self.path}'\"\n",
    "\n",
    "            self.metadata_path = self.path / \"metadata.npz\"\n",
    "\n",
    "            self._keys, self.envs = [], []\n",
    "            for db_path in db_paths:\n",
    "                self.envs.append(self.connect_db(db_path))\n",
    "                length = pickle.loads(\n",
    "                    self.envs[-1].begin().get(\"length\".encode(\"ascii\"))\n",
    "                )\n",
    "                self._keys.append(list(range(length)))\n",
    "\n",
    "            keylens = [len(k) for k in self._keys]\n",
    "            self._keylen_cumulative = np.cumsum(keylens).tolist()\n",
    "            self.num_samples = sum(keylens)\n",
    "        else:\n",
    "            self.metadata_path = self.path.parent / \"metadata.npz\"\n",
    "            self.env = self.connect_db(self.path)\n",
    "            self._keys = [\n",
    "                f\"{j}\".encode(\"ascii\")\n",
    "                for j in range(self.env.stat()[\"entries\"])\n",
    "            ]\n",
    "            self.num_samples = len(self._keys)\n",
    "\n",
    "        # If specified, limit dataset to only a portion of the entire dataset\n",
    "        # total_shards: defines total chunks to partition dataset\n",
    "        # shard: defines dataset shard to make visible\n",
    "        self.sharded = False\n",
    "        if \"shard\" in self.config and \"total_shards\" in self.config:\n",
    "            self.sharded = True\n",
    "            self.indices = range(self.num_samples)\n",
    "            # split all available indices into 'total_shards' bins\n",
    "            self.shards = np.array_split(\n",
    "                self.indices, self.config.get(\"total_shards\", 1)\n",
    "            )\n",
    "            # limit each process to see a subset of data based off defined shard\n",
    "            self.available_indices = self.shards[self.config.get(\"shard\", 0)]\n",
    "            self.num_samples = len(self.available_indices)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # if sharding, remap idx to appropriate idx of the sharded set\n",
    "        if self.sharded:\n",
    "            idx = self.available_indices[idx]\n",
    "        if not self.path.is_file():\n",
    "            # Figure out which db this should be indexed from.\n",
    "            db_idx = bisect.bisect(self._keylen_cumulative, idx)\n",
    "            # Extract index of element within that db.\n",
    "            el_idx = idx\n",
    "            if db_idx != 0:\n",
    "                el_idx = idx - self._keylen_cumulative[db_idx - 1]\n",
    "            assert el_idx >= 0\n",
    "\n",
    "            # Return features.\n",
    "            datapoint_pickled = (\n",
    "                self.envs[db_idx]\n",
    "                .begin()\n",
    "                .get(f\"{self._keys[db_idx][el_idx]}\".encode(\"ascii\"))\n",
    "            )\n",
    "            data_object = pyg2_data_transform(pickle.loads(datapoint_pickled))\n",
    "            data_object.id = f\"{db_idx}_{el_idx}\"\n",
    "        else:\n",
    "            datapoint_pickled = self.env.begin().get(self._keys[idx])\n",
    "            data_object = pyg2_data_transform(pickle.loads(datapoint_pickled))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            data_object = self.transform(data_object)\n",
    "\n",
    "        return data_object\n",
    "\n",
    "    def connect_db(self, lmdb_path=None):\n",
    "        env = lmdb.open(\n",
    "            str(lmdb_path),\n",
    "            subdir=False,\n",
    "            readonly=True,\n",
    "            lock=False,\n",
    "            readahead=False,\n",
    "            meminit=False,\n",
    "            max_readers=1,\n",
    "        )\n",
    "        return env\n",
    "\n",
    "    def close_db(self):\n",
    "        if not self.path.is_file():\n",
    "            for env in self.envs:\n",
    "                env.close()\n",
    "        else:\n",
    "            self.env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocp-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

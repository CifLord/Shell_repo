{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "ERROR:root:Invalid setup for SCN. Either the e3nn library or Jd.pt is missing.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from Models.EGformer import EGformer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ocpmodels.datasets import LmdbDataset\n",
    "from torch.utils.data import random_split\n",
    "import torch_geometric.loader as geom_loader\n",
    "import torch_geometric.data as data\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor,ModelCheckpoint\n",
    "from typing import Any\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch.nn.init as init\n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model hyperparameters to a YAML file\n",
    "model_hparams ={\"num_atoms\":0,\n",
    "                \"bond_feat_dim\":0,\n",
    "                \"num_targets\":0,\n",
    "                \"num_heads\":4,\n",
    "                }\n",
    "\n",
    "with open('params/model_hparams.yml', 'w') as file:\n",
    "    yaml.dump(model_hparams, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoTransformer_Traniner(pl.LightningModule):\n",
    "    ''''pytorch lightning'''\n",
    "    def __init__(self,model,y_mean,y_std,optimizer_name,optimizer_hparams,**model_kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model=model        \n",
    "        # self.optimizer_name=optimizer_name\n",
    "        # self.optimizer_hparams=optimizer_hparams\n",
    "        \n",
    "        self.loss_module=nn.MSELoss()\n",
    "        self.y_mean=y_mean\n",
    "        self.y_std=y_std\n",
    "\n",
    "    def forward(self,data):\n",
    "        # x,edge_index,batch_idx=data.latent,data.edge_index,data.batch\n",
    "        \n",
    "        x=self.model(data)\n",
    "        preds=x.squeeze()        \n",
    "        label=data.y_relaxed/data.natoms\n",
    "        label=(label-self.y_mean)/self.y_std\n",
    "        label=label.squeeze()\n",
    "        loss=self.loss_module(label,preds)\n",
    "        acc=abs(label-preds)\n",
    "\n",
    "        return loss,acc\n",
    "    \n",
    "    def configure_optimizers(self) -> Any:\n",
    "\n",
    "        if self.hparams.optimizer_name == \"Adam\":\n",
    "            optimizer=optim.AdamW(\n",
    "                self.parameters(),**self.hparams.optimizer_hparams\n",
    "            )\n",
    "        scheduler=optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer,milestones=[5,10],gamma=0.1\n",
    "        )    \n",
    "        # return super().configure_optimizers()\n",
    "        return [optimizer],[scheduler]\n",
    "\n",
    "\n",
    "    \n",
    "    def training_step(self,data,batch_idx):\n",
    "        loss,acc=self.forward(data)\n",
    "        self.log(\"train_loss\",loss.mean())\n",
    "        self.log(\"train_mae\",acc.mean())\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self,data,batch_idx):\n",
    "        _,acc=self.forward(data)\n",
    "        self.log(\"val_mae\",acc.mean())\n",
    "\n",
    "    def test_step(self,data,batch_idx):\n",
    "        _,acc=self.forward(data)\n",
    "        self.log(\"test_mae\",acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Unrecognized arguments: ['num_heads', 'emb_size_in', 'emb_size_trans', 'out_layer1', 'out_layer2', 'batch_size']\n",
      "c:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\torch\\nn\\init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "with open('params/model_hparams.yml', 'r') as file:\n",
    "    loaded_model_hparams = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "# Create the model using the loaded hyperparameters\n",
    "model = EGformer(**loaded_model_hparams)\n",
    "checkpoint_path='params/gemnet_oc_base_oc20_oc22.pt'\n",
    "pretrained_state_dict = torch.load(checkpoint_path)['state_dict']\n",
    "new_model_state_dict = model.state_dict()\n",
    "filtered_pretrained_state_dict = {k.strip('module.module.'): v for k, v in pretrained_state_dict.items() if k.strip('module.module.') in new_model_state_dict}\n",
    "new_model_state_dict.update(filtered_pretrained_state_dict)\n",
    "model.load_state_dict(new_model_state_dict)\n",
    "\n",
    "for param_name, param in model.named_parameters():\n",
    "\n",
    "    if param_name in filtered_pretrained_state_dict.keys():\n",
    "        \n",
    "        param.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:197: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "y_mean=-2\n",
    "y_std=2\n",
    "model=GeoTransformer_Traniner(model=model,y_mean=y_mean,y_std=y_std,optimizer_name=\"Adam\",optimizer_hparams={\"lr\":1e-3,\"weight_decay\":1e-4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeze params is 238\n",
      "Need optimiz params is 131\n"
     ]
    }
   ],
   "source": [
    "f_paras,t_paras=0,0\n",
    "for param_name,param in model.named_parameters():\n",
    "    if param.requires_grad is False:\n",
    "        f_paras+=1\n",
    "    else:\n",
    "        t_paras+=1\n",
    "print('Freeze params is',f_paras)\n",
    "print('Need optimiz params is',t_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH=\"./checkpoints\"\n",
    "# DEVICE='cuda'\n",
    "dataset=LmdbDataset({\"src\":\"Data/eoh.lmdb\"})\n",
    "\n",
    "train_length = int(0.8 * len(dataset))\n",
    "val_length = len(dataset) - train_length\n",
    "\n",
    "# Split the dataset into train and validation\n",
    "train_dataset, val_dataset =random_split(dataset, [train_length, val_length])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmoxx799\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\lhuang37\\Desktop\\Shell_trans\\Shell_repo\\wandb\\run-20230706_040403-xvbm68g1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/moxx799/Shell_repo/runs/xvbm68g1\" target=\"_blank\">stellar-fire-88</a></strong> to <a href=\"https://wandb.ai/moxx799/Shell_repo\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "# train_dataset=ReverseDataset(train_dataset,train_length)\n",
    "# val_dataset=ReverseDataset(val_dataset,val_length)\n",
    "train_loader = geom_loader.DataLoader(train_dataset, batch_size=1)\n",
    "val_loader = geom_loader.DataLoader(val_dataset, batch_size=1)\n",
    "wandb.init()\n",
    "wandb_logger = WandbLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([98])\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(train_loader))\n",
    "batched_data = data.batch\n",
    "print(batched_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,save_name=None,**kwargs):\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    if save_name is None:\n",
    "        # raise TypeError('need a save name')\n",
    "        save_name='exmodel'\n",
    "    \n",
    "    trainer=pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH,save_name),\n",
    "                       accelerator='cuda',\n",
    "                       devices=-1,\n",
    "                       max_epochs=25,\n",
    "                       callbacks=[ModelCheckpoint(save_weights_only=True,mode=\"min\",monitor=\"val_mae\"),\n",
    "                                  LearningRateMonitor(\"epoch\")],\n",
    "                       enable_progress_bar=True,\n",
    "                       logger=wandb_logger)\n",
    "    \n",
    "    trainer.logger._log_graph=True\n",
    "    trainer.logger._default_hp_metric=None\n",
    "    pretrained_filename=os.path.join(CHECKPOINT_PATH,save_name+\".ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        model=GeoTransformer_Traniner.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)\n",
    "        trainer.fit(model,train_loader,val_loader)\n",
    "        model=GeoTransformer_Traniner.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    val_result=trainer.test(model,val_loader,verbose=False)\n",
    "    # test_result=trainer.test(model,test_loader,verbose=False)\n",
    "    # result={\"test\":test_result[0][\"test_acc\"],\"val\":val_result[0][\"test_acc\"]}\n",
    "\n",
    "    return model,val_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:xvbm68g1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">stellar-fire-88</strong>: <a href=\"https://wandb.ai/moxx799/Shell_repo/runs/xvbm68g1\" target=\"_blank\">https://wandb.ai/moxx799/Shell_repo/runs/xvbm68g1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230706_040403-xvbm68g1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:xvbm68g1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\lhuang37\\Desktop\\Shell_trans\\Shell_repo\\wandb\\run-20230706_040407-1tarw2fz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/moxx799/Shell_repo/runs/1tarw2fz\" target=\"_blank\">fiery-cherry-89</a></strong> to <a href=\"https://wandb.ai/moxx799/Shell_repo\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wandb.init()\n",
    "# wandb_logger = WandbLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "c:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:44: UserWarning: attribute 'model' removed from hparams because it cannot be pickled\n",
      "  rank_zero_warn(f\"attribute '{k}' removed from hparams because it cannot be pickled\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type     | Params\n",
      "-----------------------------------------\n",
      "0 | model       | EGformer | 27.6 M\n",
      "1 | loss_module | MSELoss  | 0     \n",
      "-----------------------------------------\n",
      "5.9 M     Trainable params\n",
      "21.7 M    Non-trainable params\n",
      "27.6 M    Total params\n",
      "110.378   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lhuang37\\Desktop\\Shell_trans\\Shell_repo\\pytorch_trainer.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/pytorch_trainer.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# %%capture out\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/pytorch_trainer.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m gemformer_model,gemformer_results\u001b[39m=\u001b[39mtrain_model(model)\n",
      "\u001b[1;32mc:\\Users\\lhuang37\\Desktop\\Shell_trans\\Shell_repo\\pytorch_trainer.ipynb Cell 14\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, save_name, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/pytorch_trainer.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/pytorch_trainer.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     pl\u001b[39m.\u001b[39mseed_everything(\u001b[39m42\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/pytorch_trainer.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     trainer\u001b[39m.\u001b[39;49mfit(model,train_loader,val_loader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/pytorch_trainer.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     model\u001b[39m=\u001b[39mGeoTransformer_Traniner\u001b[39m.\u001b[39mload_from_checkpoint(trainer\u001b[39m.\u001b[39mcheckpoint_callback\u001b[39m.\u001b[39mbest_model_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/pytorch_trainer.ipynb#X16sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m val_result\u001b[39m=\u001b[39mtrainer\u001b[39m.\u001b[39mtest(model,val_loader,verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:520\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    518\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[0;32m    519\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m--> 520\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    521\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    522\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:559\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[0;32m    550\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[0;32m    551\u001b[0m )\n\u001b[0;32m    553\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    554\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    555\u001b[0m     ckpt_path,\n\u001b[0;32m    556\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    557\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    558\u001b[0m )\n\u001b[1;32m--> 559\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    561\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    562\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    932\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    933\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    934\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 935\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m    937\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    938\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    940\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:976\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m    975\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m--> 976\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[0;32m    977\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m    978\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1005\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1002\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1004\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1005\u001b[0m val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m   1007\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1009\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[0;32m    176\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:115\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m     previous_dataloader_idx \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m    114\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx)\n\u001b[0;32m    116\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:375\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[0;32m    374\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 375\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[0;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[0;32m    379\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mon_test_batch_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mon_validation_batch_end\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:288\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 288\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    291\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:378\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[0;32m    377\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, ValidationStep)\n\u001b[1;32m--> 378\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mvalidation_step(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32mc:\\Users\\lhuang37\\Desktop\\Shell_trans\\Shell_repo\\pytorch_trainer.ipynb Cell 14\u001b[0m in \u001b[0;36mGeoTransformer_Traniner.validation_step\u001b[1;34m(self, data, batch_idx)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/pytorch_trainer.ipynb#X16sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m,data,batch_idx):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/pytorch_trainer.ipynb#X16sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     _,acc\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/pytorch_trainer.ipynb#X16sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mval_mae\u001b[39m\u001b[39m\"\u001b[39m,acc\u001b[39m.\u001b[39mmean())\n",
      "\u001b[1;32mc:\\Users\\lhuang37\\Desktop\\Shell_trans\\Shell_repo\\pytorch_trainer.ipynb Cell 14\u001b[0m in \u001b[0;36mGeoTransformer_Traniner.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/pytorch_trainer.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,data):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/pytorch_trainer.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# x,edge_index,batch_idx=data.latent,data.edge_index,data.batch\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/pytorch_trainer.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/pytorch_trainer.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     preds\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39msqueeze()        \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lhuang37/Desktop/Shell_trans/Shell_repo/pytorch_trainer.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     label\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39my_relaxed\u001b[39m/\u001b[39mdata\u001b[39m.\u001b[39mnatoms\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\Desktop\\Shell_trans\\Shell_repo\\Models\\EGformer.py:425\u001b[0m, in \u001b[0;36mEGformer.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    423\u001b[0m     E_t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(E_t,dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    424\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 425\u001b[0m     num_atoms\u001b[39m=\u001b[39m[data[j]\u001b[39m.\u001b[39mnatoms[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)]\n\u001b[0;32m    426\u001b[0m \u001b[39m# for j in range(self.batch_size):\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[39m#         num_atoms.append(data[j].natoms[0])                \u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[39m# split_indices =(np.cumsum(num_atoms)[:-1])[0]\u001b[39;00m\n\u001b[0;32m    429\u001b[0m     E_t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(E_t,dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\Desktop\\Shell_trans\\Shell_repo\\Models\\EGformer.py:425\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    423\u001b[0m     E_t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(E_t,dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    424\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 425\u001b[0m     num_atoms\u001b[39m=\u001b[39m[data[j]\u001b[39m.\u001b[39mnatoms[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)]\n\u001b[0;32m    426\u001b[0m \u001b[39m# for j in range(self.batch_size):\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[39m#         num_atoms.append(data[j].natoms[0])                \u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[39m# split_indices =(np.cumsum(num_atoms)[:-1])[0]\u001b[39;00m\n\u001b[0;32m    429\u001b[0m     E_t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(E_t,dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\torch_geometric\\data\\batch.py:146\u001b[0m, in \u001b[0;36mBatch.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx: Union[\u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39minteger, \u001b[39mstr\u001b[39m, IndexType]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(idx, (\u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39minteger))\n\u001b[0;32m    144\u001b[0m             \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(idx, Tensor) \u001b[39mand\u001b[39;00m idx\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m    145\u001b[0m             \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(idx, np\u001b[39m.\u001b[39mndarray) \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39misscalar(idx))):\n\u001b[1;32m--> 146\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_example(idx)\n\u001b[0;32m    147\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(idx, \u001b[39mtuple\u001b[39m)\n\u001b[0;32m    148\u001b[0m                                   \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(idx[\u001b[39m0\u001b[39m], \u001b[39mstr\u001b[39m)):\n\u001b[0;32m    149\u001b[0m         \u001b[39m# Accessing attributes or node/edge types:\u001b[39;00m\n\u001b[0;32m    150\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getitem__\u001b[39m(idx)\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\torch_geometric\\data\\batch.py:95\u001b[0m, in \u001b[0;36mBatch.get_example\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_slice_dict\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     91\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m     92\u001b[0m         (\u001b[39m\"\u001b[39m\u001b[39mCannot reconstruct \u001b[39m\u001b[39m'\u001b[39m\u001b[39mData\u001b[39m\u001b[39m'\u001b[39m\u001b[39m object from \u001b[39m\u001b[39m'\u001b[39m\u001b[39mBatch\u001b[39m\u001b[39m'\u001b[39m\u001b[39m because \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m          \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBatch\u001b[39m\u001b[39m'\u001b[39m\u001b[39m was not created via \u001b[39m\u001b[39m'\u001b[39m\u001b[39mBatch.from_data_list()\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m---> 95\u001b[0m data \u001b[39m=\u001b[39m separate(\n\u001b[0;32m     96\u001b[0m     \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__bases__\u001b[39;49m[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m],\n\u001b[0;32m     97\u001b[0m     batch\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m     98\u001b[0m     idx\u001b[39m=\u001b[39;49midx,\n\u001b[0;32m     99\u001b[0m     slice_dict\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_slice_dict,\n\u001b[0;32m    100\u001b[0m     inc_dict\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inc_dict,\n\u001b[0;32m    101\u001b[0m     decrement\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    102\u001b[0m )\n\u001b[0;32m    104\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\torch_geometric\\data\\separate.py:37\u001b[0m, in \u001b[0;36mseparate\u001b[1;34m(cls, batch, idx, slice_dict, inc_dict, decrement)\u001b[0m\n\u001b[0;32m     35\u001b[0m         slices \u001b[39m=\u001b[39m slice_dict[attr]\n\u001b[0;32m     36\u001b[0m         incs \u001b[39m=\u001b[39m inc_dict[attr] \u001b[39mif\u001b[39;00m decrement \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     data_store[attr] \u001b[39m=\u001b[39m _separate(attr, batch_store[attr], idx, slices,\n\u001b[0;32m     38\u001b[0m                                  incs, batch, batch_store, decrement)\n\u001b[0;32m     40\u001b[0m \u001b[39m# The `num_nodes` attribute needs special treatment, as we cannot infer\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39m# the real number of nodes from the total number of nodes alone:\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(batch_store, \u001b[39m'\u001b[39m\u001b[39m_num_nodes\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\torch_geometric\\data\\separate.py:63\u001b[0m, in \u001b[0;36m_separate\u001b[1;34m(key, value, idx, slices, incs, batch, store, decrement)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Tensor):\n\u001b[0;32m     60\u001b[0m     \u001b[39m# Narrow a `torch.Tensor` based on `slices`.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[39m# NOTE: We need to take care of decrementing elements appropriately.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     cat_dim \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39m__cat_dim__(key, value, store)\n\u001b[1;32m---> 63\u001b[0m     start, end \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(slices[idx]), \u001b[39mint\u001b[39m(slices[idx \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m])\n\u001b[0;32m     64\u001b[0m     value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mnarrow(cat_dim \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m, start, end \u001b[39m-\u001b[39m start)\n\u001b[0;32m     65\u001b[0m     value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m) \u001b[39mif\u001b[39;00m cat_dim \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m value\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "# %%capture out\n",
    "\n",
    "gemformer_model,gemformer_results=train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unfinished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader,model,optimizer):\n",
    "    model.train()\n",
    "    total_loss=0.0\n",
    "    for images,masks in tqdm(data_loader):\n",
    "        images=images.to(DEVICE)\n",
    "        masks=masks.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        preds=model(images)\n",
    "        loss=\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss+=loss.item()\n",
    "        \n",
    "\n",
    "    return total_loss/len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(data_loader,model):\n",
    "\n",
    "    model.eval()\n",
    "    total_loss=0.0\n",
    "    total_acc=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for images,masks in tqdm(data_loader):\n",
    "            images=images.to(DEVICE)\n",
    "            masks=masks.to(DEVICE)        \n",
    "            logits,loss=model(images,masks)\n",
    "\n",
    "            total_loss+=loss.item() \n",
    "\n",
    "            mask=torch.sigmoid(logits[0]).cpu().squeeze().flatten()        \n",
    "            \n",
    "            mask_true=masks[0].cpu().squeeze(0).flatten()\n",
    "            \n",
    "            \n",
    "            precision=roc_auc_score(mask_true,mask)\n",
    "            total_acc+=precision\n",
    "                     \n",
    "          \n",
    "            \n",
    "\n",
    "    return total_loss/len(data_loader),total_acc/len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.005)\n",
    "best_valid_loss=np.Inf\n",
    "animator=d2l.Animator(xlabel='epoch',ylabel='loss',yscale='log',xlim=[1,epochs],ylim=[0.1,1.5],legend=['train','valid','acc'])\n",
    "\n",
    "for i in range(epochs):\n",
    "    train_loss=train_fn(trainloader,model,optimizer)\n",
    "    valid_loss,acc=eval_fn(validloader,model)\n",
    "    print(acc)\n",
    "    animator.add(i+1,(train_loss,valid_loss,acc))\n",
    "    if valid_loss< best_valid_loss:\n",
    "        torch.save(model.state_dict(),'best_model_n.pt')\n",
    "        print('saved-model')\n",
    "        best_valid_loss=valid_loss\n",
    "\n",
    "    print(f'epoch:{i+1} Train_loss:{train_loss} Valid_loss:{valid_loss} Valid acc:{acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocp-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

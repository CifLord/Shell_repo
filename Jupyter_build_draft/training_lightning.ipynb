{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from Models.EGformer import EGformer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ocpmodels.datasets import LmdbDataset\n",
    "from torch.utils.data import random_split\n",
    "import torch_geometric.loader as geom_loader\n",
    "import torch_geometric.data as data\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor,ModelCheckpoint\n",
    "from typing import Any\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "import torch_geometric.loader as geom_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict={}\n",
    "def create_model(model_name,model_hparams):\n",
    "    if model_name in model_dict: \n",
    "        return model_dict[model_name](**model_hparams)\n",
    "    else:\n",
    "        assert False, f\"Unknown model name \\\"{model_name}\\\".Available models are: {str(model_dict.keys())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_model(model):\n",
    "        checkpoint_path='params/gemnet_oc_base_oc20_oc22.pt'\n",
    "        pretrained_state_dict = torch.load(checkpoint_path)['state_dict']\n",
    "        new_model_state_dict = model.state_dict()\n",
    "        filtered_pretrained_state_dict = {k: v for k, v in pretrained_state_dict.items() if k in new_model_state_dict}\n",
    "        new_model_state_dict.update(filtered_pretrained_state_dict)\n",
    "        model.load_state_dict(new_model_state_dict)\n",
    "        for param_name, param in model.named_parameters():\n",
    "\n",
    "            if param_name in filtered_pretrained_state_dict.keys():\n",
    "                \n",
    "                param.requires_grad = False\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoTransformer_Traniner(pl.LightningModule):\n",
    "    ''''pytorch lightning'''\n",
    "    def __init__(self,model_name, model_hparams, optimizer_name, optimizer_hparams,**model_kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.optimizer_name=optimizer_name\n",
    "        self.optimizer_hparams=optimizer_hparams\n",
    "        self.model=create_model(model_name,model_hparams)\n",
    "        self.model=config_model(self.model)\n",
    "        self.loss_module=nn.MSELoss()\n",
    "\n",
    "    def forward(self,data,mode=\"train\"):\n",
    "        # x,edge_index,batch_idx=data.latent,data.edge_index,data.batch\n",
    "        \n",
    "        x=self.model(data)\n",
    "        x=x.squeeze(dim=-1)\n",
    "        preds=x.float()\n",
    "        loss=self.loss_module(data.y_relaxed,x)\n",
    "        acc=abs(data.y_relaxed-preds)\n",
    "\n",
    "        return loss,acc\n",
    "    \n",
    "    def configure_optimizers(self) -> Any:\n",
    "\n",
    "        if self.optimizer_name == \"Adam\":\n",
    "            optimizer=optim.AdamW(\n",
    "                self.parameters(),**self.hparams.optimizer_hparams\n",
    "            )\n",
    "        scheduler=optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer,milestones=[20,30],gamma=0.1\n",
    "        )    \n",
    "        # return super().configure_optimizers()\n",
    "        return [optimizer],[scheduler]\n",
    "\n",
    "\n",
    "    \n",
    "    def training_step(self,data,batch_idx):\n",
    "        loss,acc=self.forward(data,mode=\"train\")\n",
    "        self.log(\"train_loss\",loss.mean())\n",
    "        self.log(\"train_mae\",acc.mean())\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self,data,batch_idx):\n",
    "        _,acc=self.forward(data,mode=\"val\")\n",
    "        self.log(\"val_mae\",acc.mean())\n",
    "\n",
    "    def test_step(self,data,batch_idx):\n",
    "        _,acc=self.forward(data,mode=\"test\")\n",
    "        self.log(\"test_mae\",acc.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH=\"./checkpoints\"\n",
    "\n",
    "dataset=LmdbDataset({\"src\":\"Data/eoh.lmdb\"})\n",
    "\n",
    "train_length = int(0.8 * len(dataset))\n",
    "val_length = len(dataset) - train_length\n",
    "\n",
    "# Split the dataset into train and validation\n",
    "train_dataset, val_dataset =random_split(dataset, [train_length, val_length])\n",
    "# train_dataset=ReverseDataset(train_dataset,train_length)\n",
    "# val_dataset=ReverseDataset(val_dataset,val_length)\n",
    "train_loader =  geom_data.DataLoader(train_dataset, batch_size=2)\n",
    "val_loader =  geom_data.DataLoader(val_dataset, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name,save_name=None,**kwargs):\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    if save_name is None:\n",
    "        save_name=model_name\n",
    "    \n",
    "    trainer=pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH,save_name),\n",
    "                       accelerator='cuda',\n",
    "                       devices=-1,\n",
    "                       max_epochs=25,\n",
    "                       callbacks=[ModelCheckpoint(save_weights_only=True,mode=\"min\",monitor=\"val_mae\"),\n",
    "                                  LearningRateMonitor(\"epoch\")],\n",
    "                       enable_progress_bar=True,\n",
    "                       logger=wandb_logger)\n",
    "    \n",
    "    trainer.logger._log_graph=True\n",
    "    trainer.logger._default_hp_metric=None\n",
    "    pretrained_filename=os.path.join(CHECKPOINT_PATH,save_name+\".ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        model=GeoTransformer_Traniner.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)\n",
    "        model=GeoTransformer_Traniner(model_name=\"EGformer\",**kwargs)\n",
    "        trainer.fit(model,train_loader,val_loader)\n",
    "        model=GeoTransformer_Traniner.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    val_result=trainer.test(model,val_loader,verbose=False)\n",
    "    # test_result=trainer.test(model,test_loader,verbose=False)\n",
    "    # result={\"test\":test_result[0][\"test_acc\"],\"val\":val_result[0][\"test_acc\"]}\n",
    "\n",
    "    return model,val_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init()\n",
    "wandb_logger = WandbLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture out\n",
    "model_dict['EGformer']=EGformer\n",
    "gemformer_model,gemformer_results=train_model(model_name=\"EGformer\",\n",
    "                                              model_hparams={\"num_atoms\":0,\n",
    "                                                             \"bond_feat_dim\":0,\n",
    "                                                             \"num_targets\":0,\n",
    "                                                             \"num_heads\":4,\n",
    "                                                             },\n",
    "                                              optimizer_name=\"Adam\",\n",
    "                                              optimizer_hparams={\"lr\":1e-3,\n",
    "                                                                 \"weight_decay\":1e-4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE='cuda'\n",
    "# from tqdm import tqdm\n",
    "# def out_fn(dataloader,model):\n",
    "\n",
    "#     model.eval()    \n",
    "#     with torch.no_grad():\n",
    "#            for i, batch in tqdm(\n",
    "#                            enumerate(dataloader),\n",
    "#                         total=len(dataloader)\n",
    "#             ):   \n",
    "#                 batch=batch.to(DEVICE)\n",
    "#                 model=model.to(DEVICE) \n",
    "#                 output=model(batch)\n",
    "#                 print(output.shape)\n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_fn(train_loader,myGemnet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocp-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

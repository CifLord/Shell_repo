{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xubuntu/.conda/envs/ocp-models/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ocpmodels.datasets import LmdbDataset\n",
    "from torch.utils.data import random_split\n",
    "import torch_geometric.loader as geom_loader\n",
    "import torch_geometric.data as data\n",
    "from typing import Any\n",
    "import yaml\n",
    "import torch.nn.init as init\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EGformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright (c) Facebook, Inc. and its affiliates.\n",
    "This source code is licensed under the MIT license found in the\n",
    "LICENSE file in the root directory of this source tree.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import radius_graph\n",
    "from torch_scatter import scatter, segment_coo\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from ocpmodels.common.registry import registry\n",
    "from ocpmodels.common.utils import (\n",
    "    compute_neighbors,\n",
    "    conditional_grad,\n",
    "    get_max_neighbors_mask,\n",
    "    get_pbc_distances,\n",
    "    radius_graph_pbc,\n",
    "    scatter_det\n",
    "    \n",
    ")\n",
    "from ocpmodels.datasets import LmdbDataset\n",
    "from ocpmodels.modules.scaling.compat import load_scales_compat\n",
    "\n",
    "from ocpmodels.models.gemnet_oc.initializers import get_initializer\n",
    "from ocpmodels.models.gemnet_oc.interaction_indices import (\n",
    "    get_mixed_triplets,\n",
    "    get_quadruplets,\n",
    "    get_triplets,\n",
    ")\n",
    "from ocpmodels.models.gemnet_oc.layers.atom_update_block import OutputBlock\n",
    "from ocpmodels.models.gemnet_oc.layers.base_layers import Dense, ResidualLayer\n",
    "from ocpmodels.models.gemnet_oc.layers.efficient import BasisEmbedding\n",
    "from ocpmodels.models.gemnet_oc.layers.embedding_block import AtomEmbedding, EdgeEmbedding\n",
    "from ocpmodels.models.gemnet_oc.layers.force_scaler import ForceScaler\n",
    "from ocpmodels.models.gemnet_oc.layers.interaction_block import InteractionBlock\n",
    "from ocpmodels.models.gemnet_oc.layers.radial_basis import RadialBasis\n",
    "from ocpmodels.models.gemnet_oc.layers.spherical_basis import CircularBasisLayer, SphericalBasisLayer\n",
    "from ocpmodels.models.gemnet_oc.utils import (\n",
    "    get_angle,\n",
    "    get_edge_id,\n",
    "    get_inner_idx,\n",
    "    inner_product_clamped,\n",
    "    mask_neighbors,\n",
    "    repeat_blocks,\n",
    ")\n",
    "from ocpmodels.trainers import ForcesTrainer\n",
    "from ocpmodels import models\n",
    "from ocpmodels.models.gemnet_oc.gemnet_oc import GemNetOC\n",
    "from Models.encoder_layer import EncoderLayer\n",
    "\n",
    "@registry.register_model('EGformer')\n",
    "class EGformer(GemNetOC):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    num_atoms (int): Unused argument\n",
    "    bond_feat_dim (int): Unused argument\n",
    "    num_targets: int\n",
    "        Number of prediction targets.\n",
    "\n",
    "    num_spherical: int\n",
    "        Controls maximum frequency.\n",
    "    num_radial: int\n",
    "        Controls maximum frequency.\n",
    "    num_blocks: int\n",
    "        Number of building blocks to be stacked.\n",
    "\n",
    "    emb_size_atom: int\n",
    "        Embedding size of the atoms.\n",
    "    emb_size_edge: int\n",
    "        Embedding size of the edges.\n",
    "    emb_size_trip_in: int\n",
    "        (Down-projected) embedding size of the quadruplet edge embeddings\n",
    "        before the bilinear layer.\n",
    "    emb_size_trip_out: int\n",
    "        (Down-projected) embedding size of the quadruplet edge embeddings\n",
    "        after the bilinear layer.\n",
    "    emb_size_quad_in: int\n",
    "        (Down-projected) embedding size of the quadruplet edge embeddings\n",
    "        before the bilinear layer.\n",
    "    emb_size_quad_out: int\n",
    "        (Down-projected) embedding size of the quadruplet edge embeddings\n",
    "        after the bilinear layer.\n",
    "    emb_size_aint_in: int\n",
    "        Embedding size in the atom interaction before the bilinear layer.\n",
    "    emb_size_aint_out: int\n",
    "        Embedding size in the atom interaction after the bilinear layer.\n",
    "    emb_size_rbf: int\n",
    "        Embedding size of the radial basis transformation.\n",
    "    emb_size_cbf: int\n",
    "        Embedding size of the circular basis transformation (one angle).\n",
    "    emb_size_sbf: int\n",
    "        Embedding size of the spherical basis transformation (two angles).\n",
    "\n",
    "    num_before_skip: int\n",
    "        Number of residual blocks before the first skip connection.\n",
    "    num_after_skip: int\n",
    "        Number of residual blocks after the first skip connection.\n",
    "    num_concat: int\n",
    "        Number of residual blocks after the concatenation.\n",
    "    num_atom: int\n",
    "        Number of residual blocks in the atom embedding blocks.\n",
    "    num_output_afteratom: int\n",
    "        Number of residual blocks in the output blocks\n",
    "        after adding the atom embedding.\n",
    "    num_atom_emb_layers: int\n",
    "        Number of residual blocks for transforming atom embeddings.\n",
    "    num_global_out_layers: int\n",
    "        Number of final residual blocks before the output.\n",
    "\n",
    "    regress_forces: bool\n",
    "        Whether to predict forces. Default: True\n",
    "    direct_forces: bool\n",
    "        If True predict forces based on aggregation of interatomic directions.\n",
    "        If False predict forces based on negative gradient of energy potential.\n",
    "    use_pbc: bool\n",
    "        Whether to use periodic boundary conditions.\n",
    "    scale_backprop_forces: bool\n",
    "        Whether to scale up the energy and then scales down the forces\n",
    "        to prevent NaNs and infs in backpropagated forces.\n",
    "\n",
    "    cutoff: float\n",
    "        Embedding cutoff for interatomic connections and embeddings in Angstrom.\n",
    "    cutoff_qint: float\n",
    "        Quadruplet interaction cutoff in Angstrom.\n",
    "        Optional. Uses cutoff per default.\n",
    "    cutoff_aeaint: float\n",
    "        Edge-to-atom and atom-to-edge interaction cutoff in Angstrom.\n",
    "        Optional. Uses cutoff per default.\n",
    "    cutoff_aint: float\n",
    "        Atom-to-atom interaction cutoff in Angstrom.\n",
    "        Optional. Uses maximum of all other cutoffs per default.\n",
    "    max_neighbors: int\n",
    "        Maximum number of neighbors for interatomic connections and embeddings.\n",
    "    max_neighbors_qint: int\n",
    "        Maximum number of quadruplet interactions per embedding.\n",
    "        Optional. Uses max_neighbors per default.\n",
    "    max_neighbors_aeaint: int\n",
    "        Maximum number of edge-to-atom and atom-to-edge interactions per embedding.\n",
    "        Optional. Uses max_neighbors per default.\n",
    "    max_neighbors_aint: int\n",
    "        Maximum number of atom-to-atom interactions per atom.\n",
    "        Optional. Uses maximum of all other neighbors per default.\n",
    "    enforce_max_neighbors_strictly: bool\n",
    "        When subselected edges based on max_neighbors args, arbitrarily\n",
    "        select amongst degenerate edges to have exactly the correct number.\n",
    "    rbf: dict\n",
    "        Name and hyperparameters of the radial basis function.\n",
    "    rbf_spherical: dict\n",
    "        Name and hyperparameters of the radial basis function used as part of the\n",
    "        circular and spherical bases.\n",
    "        Optional. Uses rbf per default.\n",
    "    envelope: dict\n",
    "        Name and hyperparameters of the envelope function.\n",
    "    cbf: dict\n",
    "        Name and hyperparameters of the circular basis function.\n",
    "    sbf: dict\n",
    "        Name and hyperparameters of the spherical basis function.\n",
    "    extensive: bool\n",
    "        Whether the output should be extensive (proportional to the number of atoms)\n",
    "    forces_coupled: bool\n",
    "        If True, enforce that |F_st| = |F_ts|. No effect if direct_forces is False.\n",
    "    output_init: str\n",
    "        Initialization method for the final dense layer.\n",
    "    activation: str\n",
    "        Name of the activation function.\n",
    "    scale_file: str\n",
    "        Path to the pytorch file containing the scaling factors.\n",
    "\n",
    "    quad_interaction: bool\n",
    "        Whether to use quadruplet interactions (with dihedral angles)\n",
    "    atom_edge_interaction: bool\n",
    "        Whether to use atom-to-edge interactions\n",
    "    edge_atom_interaction: bool\n",
    "        Whether to use edge-to-atom interactions\n",
    "    atom_interaction: bool\n",
    "        Whether to use atom-to-atom interactions\n",
    "\n",
    "    scale_basis: bool\n",
    "        Whether to use a scaling layer in the raw basis function for better\n",
    "        numerical stability.\n",
    "    qint_tags: list\n",
    "        Which atom tags to use quadruplet interactions for.\n",
    "        0=sub-surface bulk, 1=surface, 2=adsorbate atoms.\n",
    "    latent: bool\n",
    "        Decide if output the latent space or not.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_atoms: Optional[int],\n",
    "        bond_feat_dim: int,\n",
    "        num_targets: int,\n",
    "        num_spherical=7,\n",
    "        num_radial=128,\n",
    "        num_blocks=4,\n",
    "        emb_size_atom=256,\n",
    "        emb_size_edge=512,\n",
    "        emb_size_trip_in=64,\n",
    "        emb_size_trip_out=64,\n",
    "        emb_size_quad_in=32,\n",
    "        emb_size_quad_out=32,\n",
    "        emb_size_aint_in=64,\n",
    "        emb_size_aint_out=64,\n",
    "        emb_size_rbf=16,\n",
    "        emb_size_cbf=16,\n",
    "        emb_size_sbf=32,\n",
    "        num_before_skip=2,\n",
    "        num_after_skip=2,\n",
    "        num_concat=1,\n",
    "        num_atom=3,\n",
    "        num_output_afteratom=3,\n",
    "        num_atom_emb_layers = 0,\n",
    "        num_global_out_layers = 2,\n",
    "        regress_forces = True,\n",
    "        direct_forces = False,\n",
    "        use_pbc = True,\n",
    "        scale_backprop_forces = False,\n",
    "        cutoff = 12.0,\n",
    "        cutoff_qint = 12.0,\n",
    "        cutoff_aeaint = 12.0,\n",
    "        cutoff_aint = 12.0,\n",
    "        max_neighbors = 30,\n",
    "        max_neighbors_qint =8,\n",
    "        max_neighbors_aeaint =20,\n",
    "        max_neighbors_aint = 1000,\n",
    "        enforce_max_neighbors_strictly = True,\n",
    "        rbf = {\"name\": \"gaussian\"},\n",
    "        rbf_spherical = None,\n",
    "        envelope = {\"name\": \"polynomial\", \"exponent\": 5},\n",
    "        cbf = {\"name\": \"spherical_harmonics\"},\n",
    "        sbf = {\"name\": \"spherical_harmonics\"},\n",
    "        extensive = True,\n",
    "        forces_coupled = False,\n",
    "        output_init = \"HeOrthogonal\",\n",
    "        activation = \"silu\",\n",
    "        quad_interaction = True,\n",
    "        atom_edge_interaction = True,\n",
    "        edge_atom_interaction = True,\n",
    "        atom_interaction = True,\n",
    "        scale_basis = False,\n",
    "        qint_tags = [1, 2],\n",
    "        num_elements = 83,\n",
    "        otf_graph = True,\n",
    "        scale_file = None,\n",
    "        latent: bool = True,\n",
    "        num_heads=4,\n",
    "        emb_size_in=256,\n",
    "        emb_size_trans=64,\n",
    "        out_layer1=32,\n",
    "        out_layer2=1,\n",
    "        num_attn=4,\n",
    "        **kwargs,  # backwards compatibility with deprecated arguments\n",
    "    ):\n",
    "        super().__init__(\n",
    "                        num_atoms,\n",
    "                        bond_feat_dim,\n",
    "                        num_targets,\n",
    "                        num_spherical=7,\n",
    "                        num_radial=128,\n",
    "                        num_blocks=4,\n",
    "                        emb_size_atom=256,#256\n",
    "                        emb_size_edge=512,#512\n",
    "                        emb_size_trip_in=64,\n",
    "                        emb_size_trip_out=64,\n",
    "                        emb_size_quad_in=32,\n",
    "                        emb_size_quad_out=32,\n",
    "                        emb_size_aint_in=64,\n",
    "                        emb_size_aint_out=64,\n",
    "                        emb_size_rbf=16,\n",
    "                        emb_size_cbf=16,\n",
    "                        emb_size_sbf=32,\n",
    "                        num_before_skip=2,\n",
    "                        num_after_skip=2,\n",
    "                        num_concat=1,\n",
    "                        num_atom=3,\n",
    "                        num_output_afteratom=3,\n",
    "                        num_atom_emb_layers = 0,\n",
    "                        num_global_out_layers = 2,\n",
    "                        regress_forces = True,\n",
    "                        direct_forces = False,\n",
    "                        use_pbc = True,\n",
    "                        scale_backprop_forces = False,\n",
    "                        cutoff = 12.0,\n",
    "                        cutoff_qint = 12.0,\n",
    "                        cutoff_aeaint = 12.0,\n",
    "                        cutoff_aint = 12.0,\n",
    "                        max_neighbors = 30,\n",
    "                        max_neighbors_qint =8,\n",
    "                        max_neighbors_aeaint =20,\n",
    "                        max_neighbors_aint = 1000,\n",
    "                        enforce_max_neighbors_strictly = True,\n",
    "                        rbf = {\"name\": \"gaussian\"},\n",
    "                        rbf_spherical = None,\n",
    "                        envelope = {\"name\": \"polynomial\", \"exponent\": 5},\n",
    "                        cbf = {\"name\": \"spherical_harmonics\"},\n",
    "                        sbf = {\"name\": \"spherical_harmonics\"},\n",
    "                        extensive = True,\n",
    "                        forces_coupled = False,\n",
    "                        output_init = \"HeOrthogonal\",\n",
    "                        activation = \"silu\",\n",
    "                        quad_interaction = True,\n",
    "                        atom_edge_interaction = True,\n",
    "                        edge_atom_interaction = True,\n",
    "                        atom_interaction = True,\n",
    "                        scale_basis = False,\n",
    "                        qint_tags = [1, 2],\n",
    "                        num_elements = 83,\n",
    "                        otf_graph = True,\n",
    "                        scale_file = None,\n",
    "                        num_heads=4,\n",
    "                        emb_size_in=256,\n",
    "                        emb_size_trans=64,\n",
    "                        out_layer1=64,\n",
    "                        out_layer2=1,                        \n",
    "                        num_attn=4                        \n",
    "                        )\n",
    "        \n",
    "        self.num_heads=num_heads \n",
    "        self.num_attn=num_attn       \n",
    "        self.out_layer1=out_layer1\n",
    "        self.out_layer2=out_layer2\n",
    "        self.dense1=nn.Sequential(nn.Linear(emb_size_in,emb_size_trans),\n",
    "                            nn.SiLU()\n",
    "                            )\n",
    "        self.encoder=EncoderLayer(emb_size_trans,8,emb_size_trans)\n",
    "        self.layer_norm = nn.LayerNorm(emb_size_trans)        \n",
    "        self.dense2E=nn.Sequential(nn.Linear(emb_size_trans*5,out_layer1),\n",
    "                            nn.SiLU(),\n",
    "                            nn.Linear(out_layer1,out_layer2)                                 \n",
    "                    )\n",
    "        self.dense2F=nn.Sequential(nn.Linear(emb_size_trans,out_layer1),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(out_layer1,out_layer2)                                 \n",
    "                    )\n",
    "\n",
    "    def forward(self, data):\n",
    "        pos = data.pos\n",
    "        batch = data.batch\n",
    "        #print(len(data))    \n",
    "        atomic_numbers = data.atomic_numbers.long()\n",
    "        \n",
    "        num_atoms = atomic_numbers.shape[0]\n",
    "        if self.regress_forces and not self.direct_forces:\n",
    "            pos.requires_grad_(True)\n",
    "\n",
    "        (\n",
    "            main_graph,\n",
    "            a2a_graph,\n",
    "            a2ee2a_graph,\n",
    "            qint_graph,\n",
    "            id_swap,\n",
    "            trip_idx_e2e,\n",
    "            trip_idx_a2e,\n",
    "            trip_idx_e2a,\n",
    "            quad_idx,\n",
    "        ) = self.get_graphs_and_indices(data)\n",
    "        # print('checkpoint1')\n",
    "        _, idx_t = main_graph[\"edge_index\"]\n",
    "\n",
    "        (\n",
    "            basis_rad_raw,\n",
    "            basis_atom_update,\n",
    "            basis_output,\n",
    "            bases_qint,\n",
    "            bases_e2e,\n",
    "            bases_a2e,\n",
    "            bases_e2a,\n",
    "            basis_a2a_rad,\n",
    "        ) = self.get_bases(\n",
    "            main_graph=main_graph,\n",
    "            a2a_graph=a2a_graph,\n",
    "            a2ee2a_graph=a2ee2a_graph,\n",
    "            qint_graph=qint_graph,\n",
    "            trip_idx_e2e=trip_idx_e2e,\n",
    "            trip_idx_a2e=trip_idx_a2e,\n",
    "            trip_idx_e2a=trip_idx_e2a,\n",
    "            quad_idx=quad_idx,\n",
    "            num_atoms=num_atoms,\n",
    "        )\n",
    "        # Embedding block\n",
    "        h = self.atom_emb(atomic_numbers)\n",
    "        # (nAtoms, emb_size_atom)\n",
    "        m = self.edge_emb(h, basis_rad_raw, main_graph[\"edge_index\"])\n",
    "        # (nEdges, emb_size_edge)\n",
    "        \n",
    "        x_E, x_F = self.out_blocks[0](h, m, basis_output, idx_t)\n",
    "        \n",
    "        xs_E, xs_F = [x_E], [x_F]\n",
    "       \n",
    "        # (nAtoms, num_targets), (nEdges, num_targets)\n",
    "\n",
    "        for i in range(self.num_blocks):\n",
    "            # Interaction block\n",
    "            h, m = self.int_blocks[i](\n",
    "                h=h,\n",
    "                m=m,\n",
    "                bases_qint=bases_qint,\n",
    "                bases_e2e=bases_e2e,\n",
    "                bases_a2e=bases_a2e,\n",
    "                bases_e2a=bases_e2a,\n",
    "                basis_a2a_rad=basis_a2a_rad,\n",
    "                basis_atom_update=basis_atom_update,\n",
    "                edge_index_main=main_graph[\"edge_index\"],\n",
    "                a2ee2a_graph=a2ee2a_graph,\n",
    "                a2a_graph=a2a_graph,\n",
    "                id_swap=id_swap,\n",
    "                trip_idx_e2e=trip_idx_e2e,\n",
    "                trip_idx_a2e=trip_idx_a2e,\n",
    "                trip_idx_e2a=trip_idx_e2a,\n",
    "                quad_idx=quad_idx,\n",
    "            )  # (nAtoms, emb_size_atom), (nEdges, emb_size_edge)\n",
    "\n",
    "            x_E,x_F= self.out_blocks[i + 1](h, m, basis_output, idx_t)\n",
    "            # (nAtoms, emb_size_atom), (nEdges, emb_size_edge)\n",
    "            xs_E.append(x_E)\n",
    "            xs_F.append(x_F)\n",
    "        # Previous is the Gemnet part, and the following is the Transformer part\n",
    "        E_t = torch.stack(xs_E, dim=-1)\n",
    "        #(E_t.shape==n_batch x 256 x 5)\n",
    "        E_t=E_t.permute(0,2,1)\n",
    "        E_t = self.dense1(E_t)\n",
    "        E_t = self.layer_norm(E_t)\n",
    "        for _ in range(self.num_attn):             \n",
    "            E_t, _ = self.encoder(E_t, mask=None)\n",
    "            E_t = self.layer_norm(E_t)\n",
    "        E_t=E_t.reshape(E_t.shape[0],-1)\n",
    "        E_t=self.dense2E(E_t)\n",
    "        #F_t=self.dense2F(E_t)                  \n",
    "        #(E_t.shape)=n_atoms*1*1\n",
    "        nMolecules = torch.max(batch) + 1        \n",
    "        E_t = scatter_det(\n",
    "            E_t, batch, dim=0, dim_size=nMolecules, reduce=\"add\"\n",
    "        )\n",
    "\n",
    "        return E_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model hyperparameters to a YAML file\n",
    "# model_hparams ={\"num_atoms\":0,\n",
    "#                 \"bond_feat_dim\":0,\n",
    "#                 \"num_targets\":0,\n",
    "#                 \"num_heads\":4,\n",
    "#                 \"batch_size\":4\n",
    "#                 }\n",
    "\n",
    "# with open('params/model_hparams.yml', 'w') as file:\n",
    "#     yaml.dump(model_hparams, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('params/model_hparams.yml', 'r') as file:\n",
    "    hyper_config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "warmup_epochs = hyper_config['configs'].get(\"warmup_epochs\")\n",
    "decay_epochs = hyper_config['configs'].get(\"decay_epochs\")\n",
    "y_mean = hyper_config['configs'].get(\"y_mean\")\n",
    "y_std = hyper_config['configs'].get(\"y_std\")\n",
    "num_epochs =hyper_config['configs'].get(\"num_epochs\")\n",
    "batch_size = hyper_config['configs'].get(\"batch_size\")\n",
    "learning_rate = hyper_config['configs'].get(\"learning_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('params/model_hparams.yml', 'r') as file:\n",
    "#     loaded_model_hparams = yaml.load(file, Loader=yaml.FullLoader)['model']\n",
    "# # Create the model using the loaded hyperparameters\n",
    "# model = EGformer(**loaded_model_hparams)\n",
    "# checkpoint_path='params/best_model_all.pt'\n",
    "# pretrained_state_dict = torch.load(checkpoint_path)['MODEL_STATE']\n",
    "# model.load_state_dict(pretrained_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Unrecognized arguments: ['num_heads', 'emb_size_in', 'emb_size_trans', 'out_layer1', 'out_layer2', 'num_attn']\n"
     ]
    }
   ],
   "source": [
    "# with open('params/model_hparams.yml', 'r') as file:\n",
    "#     loaded_model_hparams = yaml.load(file, Loader=yaml.FullLoader)['model']\n",
    "# Create the model using the loaded hyperparameters\n",
    "loaded_model_hparams ={\"num_atoms\":0,\n",
    "                \"bond_feat_dim\":0,\n",
    "                \"num_targets\":1,\n",
    "                \"num_heads\":4,\n",
    "                \n",
    "                }\n",
    "model = EGformer(**loaded_model_hparams)\n",
    "checkpoint_path=os.path.join('params', 'gemnet_oc_base_oc20_oc22.pt')\n",
    "pretrained_state_dict = torch.load(checkpoint_path)['state_dict']\n",
    "new_model_state_dict = model.state_dict()\n",
    "filtered_pretrained_state_dict = {k.strip('module.module.'): v for k, v in pretrained_state_dict.items() if k.strip('module.module.') in new_model_state_dict}\n",
    "new_model_state_dict.update(filtered_pretrained_state_dict)\n",
    "model.load_state_dict(new_model_state_dict)\n",
    "for param_name, param in model.named_parameters():\n",
    "    if param_name in filtered_pretrained_state_dict.keys():                \n",
    "        param.requires_grad = False     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeze params is 238\n",
      "Need optimiz params is 159\n"
     ]
    }
   ],
   "source": [
    "f_paras,t_paras=0,0\n",
    "for param_name,param in model.named_parameters():\n",
    "    if param.requires_grad is False:\n",
    "        f_paras+=1\n",
    "    else:\n",
    "        t_paras+=1\n",
    "print('Freeze params is',f_paras)\n",
    "print('Need optimiz params is',t_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_epochs=2\n",
    "decay_epochs=3\n",
    "    \n",
    "y_mean=-7\n",
    "y_std=6\n",
    "num_epochs=10\n",
    "batch_size = 6\n",
    "learning_rate=0.001\n",
    "CHECKPOINT_PATH=\"./checkpoints\"\n",
    "DEVICE=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset=LmdbDataset({\"src\":\"/shareddata/ocp/ocp22/oc22_trajectories/trajectories/Transformer_clean_valid/data.0000.lmdb\"})\n",
    "\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "warmup_scheduler = optim.lr_scheduler.LambdaLR(optimizer,lr_lambda=lambda epoch: (epoch+1)/warmup_epochs)\n",
    "decay_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=decay_epochs,gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = int(0.8 * len(dataset))\n",
    "val_length = len(dataset) - train_length\n",
    "\n",
    "# Split the dataset into train and validation\n",
    "train_dataset, val_dataset =random_split(dataset, [train_length, val_length])\n",
    "train_loader = geom_loader.DataLoader(train_dataset, batch_size=4,drop_last=True)\n",
    "val_loader = geom_loader.DataLoader(val_dataset, batch_size=4,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([311])\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(train_loader))\n",
    "batched_data = data.batch\n",
    "print(batched_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test of model\n",
    "test_input=model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 5872834\n"
     ]
    }
   ],
   "source": [
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Number of trainable parameters: {num_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-33.6354],\n",
       "        [-22.4121],\n",
       "        [-34.7020],\n",
       "        [-26.5363]], grad_fn=<ScatterAddBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(predictions, targets, y_mean=-7, y_std=6):\n",
    "    masks = (targets.y/ targets.natoms - y_mean) / y_std\n",
    "    mask_loss = nn.MSELoss()\n",
    "    mask_acc=nn.L1Loss()\n",
    "    loss = mask_loss(predictions.view(-1, 1), masks.view(-1, 1))\n",
    "    accuracy = mask_acc(predictions.view(-1, 1) , masks.view(-1, 1))\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device, optimize_after=8):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    iteration = 0\n",
    "\n",
    "    for images in tqdm(data_loader):\n",
    "        model=model.to(device)\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(images)\n",
    "        targets = images\n",
    "        loss, acc = get_loss(predictions, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # Accumulate gradients for a specified number of iterations\n",
    "        iteration += 1\n",
    "        if iteration % optimize_after == 0:\n",
    "            optimizer.step()\n",
    "            iteration = 0\n",
    "            total_loss += loss.item()\n",
    "        break\n",
    "\n",
    "    return total_loss / (len(data_loader) // optimize_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(data_loader,model,device):\n",
    "    model.eval()\n",
    "    total_loss=0.0\n",
    "    total_acc=0    \n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(data_loader):\n",
    "            model=model.to(device) \n",
    "            images=images.to(device)\n",
    "            predictions = model(images)\n",
    "            targets = images            \n",
    "            loss, acc =get_loss(predictions, targets)\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc.item()\n",
    "\n",
    "    return total_loss/len(data_loader),total_acc/len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss=np.Inf\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss=train_fn(train_loader,model,optimizer,device=DEVICE)\n",
    "    valid_loss,acc=eval_fn(val_loader,model,device=DEVICE)\n",
    "    if epoch<warmup_epochs:\n",
    "        warmup_scheduler.step()\n",
    "    else:\n",
    "        decay_scheduler.step()    \n",
    "    if valid_loss< best_valid_loss:\n",
    "        torch.save(model.state_dict(),'best_model_n.pt')\n",
    "        print('saved-model')\n",
    "        best_valid_loss=valid_loss\n",
    "    current_lr=optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    print(f'epoch:{epoch+1} Train_loss:{train_loss} Valid_loss:{valid_loss} Valid acc:{acc} lr:{current_lr}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocp-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

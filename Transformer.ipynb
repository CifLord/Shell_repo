{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lhuang37\\.conda\\envs\\ocp-models\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch_scatter import scatter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Positional_embedding(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gemformer(nn.Module):\n",
    "    '''This is a Graph Transformer neural network, \n",
    "    the input is the Gemnet-oc latent space with the best pre-trained params\n",
    "    \n",
    "    Parameters\n",
    "    num_heads:\n",
    "        Number of heads\n",
    "    '''\n",
    "\n",
    "    def __init__(self,num_heads,emb_size_in,emb_size_trans,out_layer1=32,out_layer2=1):\n",
    "        super(Gemformer, self).__init__()\n",
    "        self.num_heads=num_heads\n",
    "        \n",
    "        self.out_layer1=out_layer1\n",
    "        self.out_layer2=out_layer2\n",
    "        self.dense=nn.Sequential(nn.Linear(emb_size_trans,out_layer1),\n",
    "                                 nn.SiLU(),\n",
    "                                 nn.Linear(out_layer1,out_layer2)                                 \n",
    "                                 )\n",
    "\n",
    "        self.lin_query_MHA=nn.Linear(emb_size_in,emb_size_trans)\n",
    "        self.lin_key_MHA=nn.Linear(emb_size_in,emb_size_trans)\n",
    "        self.lin_value_MHA=nn.Linear(emb_size_in,emb_size_trans)\n",
    "\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "\n",
    "        self.MHA=nn.MultiheadAttention(embed_dim=emb_size_trans,\n",
    "                                       num_heads=num_heads,\n",
    "                                       bias=True,\n",
    "                                       dropout=0.0,\n",
    "                                       )\n",
    "        self.layer_norm = nn.LayerNorm(emb_size_trans)\n",
    "        \n",
    "    def check_shape(self,va):\n",
    "        '''check the varaible shape,mean and std,\n",
    "           only for develop the model'''\n",
    "        print(f'The {va.__class__.__name__} shape is:',va.shape)\n",
    "        print(f'The {va.__class__.__name__} mean and std of is:',va.mean(),va.std())\n",
    "        \n",
    "\n",
    "    def forward(self,data):\n",
    "\n",
    "        E_all=data.latent\n",
    " \n",
    "        batch = data.batch\n",
    "        # print(batch)\n",
    "        q=self.lin_query_MHA(E_all)\n",
    "        k=self.lin_key_MHA(E_all)\n",
    "        v=self.lin_value_MHA(E_all)\n",
    "\n",
    "        nMolecules = torch.max(batch) + 1\n",
    "        E_t,w=self.MHA(q,k,v)\n",
    "        # self.check_shape(E_t)\n",
    "        E_t=torch.sum(E_t,dim=0)\n",
    "        # self.check_shape(E_t)\n",
    "        E_t = self.layer_norm(E_t)\n",
    "        # self.check_shape(E_t)\n",
    "        \n",
    "        E_t = scatter(\n",
    "                E_t, batch, dim=0, dim_size=nMolecules, reduce=\"add\"\n",
    "            )  # (nMolecules, num_targets)\n",
    "        # self.check_shape(E_t)\n",
    "        \n",
    "        E_t=self.dense(E_t)\n",
    "        # self.check_shape(E_t)\n",
    "\n",
    "        \n",
    "        return E_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE='cuda'\n",
    "def out_fn(dataloader,model):\n",
    "\n",
    "    model.eval()    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            \n",
    "            data=data.to(DEVICE)  \n",
    "            model=model.to(DEVICE) \n",
    "            output=model(data)\n",
    "            break\n",
    "            \n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTransformer=Gemformer(1,256,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tensor shape is: torch.Size([1, 64])\n",
      "The Tensor mean and std of is: tensor(-1.0133e-06, device='cuda:0') tensor(54.8691, device='cuda:0')\n",
      "The Tensor shape is: torch.Size([1, 1])\n",
      "The Tensor mean and std of is: tensor(0.6330, device='cuda:0') tensor(nan, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from ocpmodels.datasets import LmdbDataset\n",
    "dataset=LmdbDataset({\"src\":\"Data/eoh_t.lmdb\"})\n",
    "import torch_geometric.loader as geom_loader\n",
    "node_data_loader = geom_loader.DataLoader(dataset, batch_size=1)\n",
    "output=out_fn(node_data_loader,myTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1]), tensor([[0.6330]], device='cuda:0'))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling_file=torch.load('params/gemnet_oc_base_oc20_oc22_scales.pt')\n",
    "# scaling_file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocp-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
